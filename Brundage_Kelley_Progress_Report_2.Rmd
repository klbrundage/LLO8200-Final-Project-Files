---
title: "Progress Report 2"
Date: 12/June/2019
GitHub: https://github.com/klbrundage/LLO8200-Assignments
output:
  pdf_document: default
  html_document: default
fontsize: 12pt
geometry: margin=.5
autosize: yes
Author: Kelley Brundage
---

```{r setup, include=FALSE}
##This code allows the Knit function to still work even with errors 
knitr::opts_chunk$set(echo=TRUE,error=TRUE)
```

```{r global_options, include = FALSE}
##This code does not show in the final document but will assist with definining the margin cutoff point and wraps the text to the next line.
knitr::opts_chunk$set(message=FALSE, 
tidy.opts=list(width.cutoff=60)) 

my_pdf = function(file,width,height)
  {pdf(file, width=width, height=height,pointsize=12)}
```

```{r eval=F, tidy=T}
##The `eval=FALSE` option just displays the R code (and does not run it), `tidy=TRUE` wraps long code so it does not run off the page.

##Below are the standard set of setup commands and common libraries that may be needed when running data analysis as well as supporting table design and setup.
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library(dplyr))
suppressMessages(library(evaluate))
suppressMessages(library(forcats))
suppressMessages(library(formatR))
suppressMessages(library(ggplot2))
suppressMessages(library(haven))
suppressMessages(library(knitr))
suppressMessages(library(ModelMetrics))
suppressMessages(library(modelr))
suppressMessages(library(pander))
suppressMessages(library(readxl))
suppressMessages(library(RColorBrewer))
suppressMessages(library(stargazer))
suppressMessages(library(tables))
suppressMessages(library(tibble))
suppressMessages(library(tidyverse))
suppressMessages(library(xtable))
```

```{r Import World Ranking Datasets}
##The code below will upload the World University Ranking Datasets obtained from Kaggle.com

##The next set of files are the three World Ranking system files
library(readxl)
cwur <- read_excel("~/School/EdD program - Vanderbilt/01 - Classes/LLO 8200 - Intro to Data Science/Final Project/datafiles/cwurData.xlsx")

save(cwur, file = "cwurData.xlsx") #save as excel file name cwurData.xlsx

library(readxl)
shanghai <- read_excel("~/School/EdD program - Vanderbilt/01 - Classes/LLO 8200 - Intro to Data Science/Final Project/datafiles/shanghaiData.xlsx")

save(shanghai, file = "shanghaiData.xlsx") #save as excel file name shanghaiData.xlsx

library(readxl)
times <- read_excel("~/School/EdD program - Vanderbilt/01 - Classes/LLO 8200 - Intro to Data Science/Final Project/datafiles/timesData.xlsx")

save(times, file = "timesData.xlsx") #save as excel file name timesData.xlsx

##The three datasets below are the additional datasets provided that connect to the World University Ranking datasets
library(readxl)
education_expenditure_supplementary_data <- read_excel("~/School/EdD program - Vanderbilt/01 - Classes/LLO 8200 - Intro to Data Science/Final Project/datafiles/education_expenditure_supplementary_data.xlsx")

library(readxl)
educational_attainment_supplementary_data <- read_excel("~/School/EdD program - Vanderbilt/01 - Classes/LLO 8200 - Intro to Data Science/Final Project/datafiles/educational_attainment_supplementary_data.xlsx")

##The file below lists the name of the institution and the Country the institution is located in
library(readxl)
school_and_country_table <- read_excel("~/School/EdD program - Vanderbilt/01 - Classes/LLO 8200 - Intro to Data Science/Final Project/datafiles/school_and_country_table.xlsx")
```

# Progress Report 2:  World University Rankings

##*Synopsis of Problem & Approach from Progress Report 1*
Compare the three global ranking systems to the amount of faculty, publications/research at each institution per ranking system by approaching each dataset with an analytical and statistical viewpoint. 

Analyze the dataset specific to the area of research/academic and if common challenges that exist with all ranking systems exist.  Problems such as making the correction for institutional size, differences between average and extreme, defining the institutions, measurement of time frame, credit allocation, excellency factors as well as adjustment for scientific fields or types of research.

##*Columns in each dataset that will be used to compare and analysze the ranking system*

CWUR Dataset Fields:
world_rank; institution; country; national_rank; publications; citations
#Quality of Faculty, measured by the number of academics who have won major international awards, prizes, and medals (15%)

#Quality Publications, measured by the number of research papers appearing in top-tier journals (15%) 

Shanghai Dataset Fields:
word_rank; university_name; national_rank; pub; hici

Times Dataset Fields:
world_rank; university; country; research; citations
#Research (volume, income, and reputation)

#Citations (research influence)

# **Extensive Investigation of Dataset**

Investigate data: distribution of data, correlations, associations, and predictive potential to solve your proposed problem:

Continued review and analysis of the datasets led me to a series of common fields within the three primary sets: CWUR, Shanghai and Times.  All three hold common columns such as the World Rank, Institution/University Name, Publications/Research and Citations.  Analysis will expand to look at both the research and citations between the three ranking systems.

###Setting up the data frame

```{r Tidy Principles}
#1. Each variable forms a column
#2. Each observation forms a row
#3. Each type of observational units forms a table

is.data.frame(cwur)
is.tibble(cwur)
is_tibble(cwur)
typeof(cwur)

is.data.frame(shanghai)
is.tibble(shanghai)
is_tibble(shanghai)
typeof(shanghai)

is.data.frame(times)
is.tibble(times)
is_tibble(times)
typeof(times)
```



###*Data Clean-up*

```{r CWUR Data Column labels}

## This code will clean up the column labels to align directly with the variable definitions for the columns

names(cwur)<-c("World Rank",
                "Instiution Name",
                "Country",
                "National Rank",
                "Quality of Education",
                "Alumni Employment",
                "Quality of Faculty",
                "Publications",
                "Influence",
                "Citations",
                "Broad Impact",
                "Patents",
                "Score",
                "Year")

head(cwur)
```

```{r Shanghai Data Column labels}

## This code will clean up the column labels to align directly with the variable definitions for the columns

names(shanghai)<-c("World Rank",
                "University Name",
                "National Rank",
                "Total Score",
                "Alumni",
                "Award",
                "Highly Cited Researchers",
                "Nature & Science Pubs",
                "Publications",
                "Per Capita Performance",
                "Year")

head(shanghai)
```

```{r Times Data Column labels}

## This code will clean up the column labels to align directly with the variable definitions for the columns 

names(times)<-c("World Rank",
                "University Name",
                "Country",
                "Teaching",
                "International",
                "Research",
                "Citations",
                "Income",
                "Total Score",
                "Number of Students",
                "Student/Staff Ratio",
                "International Students",
                "Female/Male Ratio",
                "Year")

head(times)
```


Below you will find the mean of overall publications/citations for the three global ranking systems.  You will find that for all three datasets; CWUR, Shanghai and Times that on average 50 publications are produced.  The same holds true for cititations in that on average 50 citations are produced per institution.


```{r Regression for CWUR Data - Publications}
cwurp_mean <- cwur%>%summarise(mean(cwur$Publications,na.rm = T))
cwurp_mean
  #na.rm=T specifies what to do with any missing data in the dataset

cwur <- cwur%>%mutate(publications_p=percent_rank(cwur$Publications)*100)
  #default would give 0-1 so *by 100 and it will make it easier to understand the publication number in the CWUR dataset

cwurpp_mean <- cwur%>%summarise(mean(publications_p,na.rm = T))
cwurpp_mean
  #gives the mean of the publications in the CWUR dataset

#The code below shows Publications by Country in the CWUR dataset showing the mean, standard deviation, max and min.
desc_cwur <- cwur%>%
  group_by(Country)%>%
  summarise(mean_pub = mean(publications_p),
            sd_pub = sd(publications_p),
            max_pub= max(publications_p),
            min_pub= min(publications_p))
desc_cwur

#The code below shows which country has the maximum and minimum Publications inthe CWUR dataset
desc_cwur%>%
  filter(max(mean_pub) == mean_pub)

desc_cwur%>%
  filter(min(mean_pub) == mean_pub)
```

```{r Regression for CWUR Data - Citations}

cwurc_mean <- cwur%>%summarise(mean(cwur$Citations,na.rm = T))
cwurc_mean
  #na.rm=T specifies what to do with any missing data in the dataset

cwur <- cwur%>%mutate(citations_p=percent_rank(cwur$Citations)*100)
  #default would give 0-1 so *by 100 and it will make it easier to understand the publication number in the CWUR dataset

cwurcp_mean <- cwur%>%summarise(mean(citations_p,na.rm = T))
cwurcp_mean
  #gives the mean of the publications in the CWUR dataset

#The code below shows Citations by Country in the CWUR dataset showing the mean, standard deviation, max and min.
desc_cwur <- cwur%>%
  group_by(Country)%>%
  summarise(mean_cit = mean(Citations),
            sd_cit = sd(Citations),
            max_cit= max(Citations),
            min_cit= min(Citations))
desc_cwur

#The code below shows which country has the maximum and minimum Citations inthe CWUR dataset
desc_cwur%>%
  filter(max(mean_cit) == mean_cit)

desc_cwur%>%
  filter(min(mean_cit) == mean_cit)
```


```{r Regression for Shanghai Data - Publications}

spub_mean <- shanghai%>%summarise(mean(shanghai$Publications,na.rm = T))
spub_mean
  #na.rm=T specifies what to do with any missing data in the dataset

shanghai <- shanghai%>%mutate(publications_p=percent_rank(shanghai$Publications)*100)
  #default would give 0-1 so *by 100 and it will make it easier to understand the publication number in the Shanghai dataset

spubp_mean <- shanghai%>%summarise(mean(publications_p,na.rm = T))
spubp_mean
  #gives the mean of the publications in the Shanghai dataset

#The code below shows Citations by National Rank in the Shanghai dataset showing the mean, standard deviation, max and min.
desc_shanghai <- shanghai%>%
  group_by(shanghai$`National Rank`)%>%
  summarise(mean_spub = mean(shanghai$Publications),
            sd_spub = sd(shanghai$Publications),
            max_spub = max(shanghai$Publications),
            min_spub = min(shanghai$Publications))
desc_shanghai

#The code below shows which country has the maximum and minimum Citations in the Shanghai dataset
desc_shanghai%>%
  filter(max(mean_spub) == mean_spub)

desc_shanghai%>%
  filter(min(mean_spub) == mean_spub)
```

```{r Regression for Shanghai Data - Highly Cited Researchers}

shici_mean <- shanghai%>%summarise(mean(shanghai$`Highly Cited Researchers`,na.rm = T))
shici_mean
  #na.rm=T specifies what to do with any missing data in the dataset

shanghai <- shanghai%>%mutate(hici_p=percent_rank(shanghai$`Highly Cited Researchers`)*100)
  #default would give 0-1 so *by 100 and it will make it easier to understand the publication number in the Shanghai dataset

shicip_mean <- shanghai%>%summarise(mean(hici_p,na.rm = T))
shicip_mean
  #gives the mean of the publications in the Shanghai dataset

#The code below shows Citations by National Rank in the Shanghai dataset showing the mean, standard deviation, max and min.
desc_shanghai <- shanghai%>%
  group_by(shanghai$`National Rank`)%>%
  summarise(mean_shici = mean(shanghai$`Highly Cited Researchers`),
            sd_shici = sd(shanghai$`Highly Cited Researchers`),
            max_shici = max(shanghai$`Highly Cited Researchers`),
            min_shici = min(shanghai$`Highly Cited Researchers`))
desc_shanghai

#The code below shows which country has the maximum and minimum Citations in the Shanghai dataset
desc_shanghai%>%
  filter(max(mean_shici) == mean_shici)

desc_shanghai%>%
  filter(min(mean_shici) == mean_shici)
```

```{r Regression for Times Data - Publications/Research}

timesr_mean <- times%>%summarise(mean(times$Research,na.rm = T))
timesr_mean
  #na.rm=T specifies what to do with any missing data in the dataset

times <- times%>%mutate(research_p=percent_rank(times$Research)*100)
  #default would give 0-1 so *by 100 and it will make it easier to understand the publication number in the Times dataset

timesrp_mean <- times%>%summarise(mean(research_p,na.rm = T))
timesrp_mean
  #gives the mean of the publications in the Times dataset

#The code below shows Publications/Research by Country in the Times dataset showing the mean, standard deviation, max and min.
desc_times <- times%>%
  group_by(Country)%>%
  summarise(mean_tpub = mean(times$Research),
            sd_tpub = sd(times$Research),
            max_tpub= max(times$Research),
            min_tpub= min(times$Research))
desc_times

#The code below shows which country has the maximum and minimum Publications/Research in the Times dataset
desc_times%>%
  filter(max(mean_tpub) == mean_tpub)

desc_times%>%
  filter(min(mean_tpub) == mean_tpub)
```


```{r Regression for Times Data - Citations}

timesc_mean <- times%>%summarise(mean(times$Citations,na.rm = T))
timesc_mean
  #na.rm=T specifies what to do with any missing data in the dataset

times <- times%>%mutate(citations_p=percent_rank(times$Citations)*100)
  #default would give 0-1 so *by 100 and it will make it easier to understand the publication number in the Times dataset

timescp_mean <- times%>%summarise(mean(citations_p,na.rm = T))
timescp_mean
  #gives the mean of the publications in the Times dataset

#The code below shows Publications/Research by Country in the Times dataset showing the mean, standard deviation, max and min.
desc_times <- times%>%
  group_by(Country)%>%
  summarise(mean_tcit = mean(times$Citations),
            sd_tcit = sd(times$Citations),
            max_tcit= max(times$Citations),
            min_tcit= min(times$Citations))
desc_times

#The code below shows which country has the maximum and minimum Publications/Research in the Times dataset
desc_times%>%
  filter(max(mean_tcit) == mean_tcit)

desc_times%>%
  filter(min(mean_tcit) == mean_tcit)
```

Summary of Regression Data Above

```{r Summary of three ranking systems regression data}
cwurp_mean
cwurpp_mean
cwurc_mean
cwurcp_mean
desc_cwur

spub_mean
spubp_mean
shici_mean
shicip_mean
desc_shanghai

timesr_mean
timesrp_mean
timesc_mean
timescp_mean
desc_times
```


Below are a baseline set of Density plots that show the regression data for overall publications and citations.  

```{r Density Plot of Citations in CWUR Dataset}

gc1 <- ggplot(cwur,aes(x=cwur$Citations))
gc1 <- gc1+geom_density(binwidth = 1,fill="lightblue") #Density is the shape
gc1 <- gc1+ylab("Count")+xlab("Citations") ## x & y axis labels
gc1 <- gc1+theme(axis.text.x = element_text(angle = 60, hjust = 1))
gc1 <- gc1+ggtitle("Citations in CWUR Dataset") ## Chart Title
gc1
```

```{r Density Plot of Citations in Shanghai Dataset}
gs1 <- ggplot(shanghai,aes(x=shanghai$`Highly Cited Researchers`))
gs1 <- gs1+geom_density(binwidth = 1,fill="lightgreen") #Density is the shape
gs1 <- gs1+ylab("Count")+xlab("Citations") ## x & y axis labels
gs1 <- gs1+theme(axis.text.x = element_text(angle = 60, hjust = 1))
gs1 <- gs1+ggtitle("Citations in Shanghai Dataset") ## Chart Title
gs1
```

```{r Density Plot of Citations in Times Dataset}

gt1 <- ggplot(times,aes(x=times$Citations))
gt1 <- gt1+geom_density(binwidth = 1,fill="lightyellow") #Density is the shape
gt1 <- gt1+ylab("Count")+xlab("Citations") ## x & y axis labels
gt1 <- gt1+theme(axis.text.x = element_text(angle = 60, hjust = 1))
gt1 <- gt1+ggtitle("Citations in Times Dataset") ## Chart Title
gt1
```

##*World Rankings Dataset Predictions*

The data and charts below show the pulication/citation predictions based on the three world datasets.

##CWUR Dataset Prediction
```{r CWUR Publications Prediction}

cpub1<-lm(cwur$Publications~cwur$Citations,data=cwur) 
  #publications ~(as a function of) citations, data=dataset(cwur)

#outcome (publications) on left, predictor (citations) on right 
summary(cpub1)#shows the results of the regression
```

RESULTS:
If the Citations for the CWUR Dataset is 0 then the intercept(publications) are predicted to be 65.68 (Est. Std).
As citations increases within the CWUR Dataset (every 1 unit change) publications are predicted to increase by .95 points (Est. Std)

Fail to Reject the Null Hypothesis.

Residual Standard Error (RMSE): 169.5 on df(2198)


```{r Point Plot of CWUR Publications by Citations}
#The graph/plot below is pulling from the main dataset - CWUR in order to present the best point plot

gc2 <- ggplot(cwur,aes(x=cwur$Citations, y=cwur$Publications))+
  geom_point(shape=2, alpha=.5, size=.5)+
 geom_smooth(method = "lm")+
  geom_smooth(method = "loess",color="darkblue")+
  geom_smooth(color="grey")
gc2 <- gc2+ylab("Publications")+xlab("Citations")
gc2 <- gc2+ggtitle("Regression Lines: Number of Publications by Citations made in CWUR Dataset")
gc2

##lm puts a straight line through the publication data points and is similiar to a best line fit.
##LOESS fits a curve through the publication data points and is similar to modeling with calculus as it is the weighted sum of squared erros and may accurately account for the range within the dataset.

```

Based on the data below the Root Mean Squared Error for the CWUR prediction data for publications is 169.43, which means on average we are off by 169.

```{r RMSE for CWUR publications}
#The code below runs the root mean squared error number from a validation of the model data above

cwur<- cwur%>%add_predictions(cpub1)%>%rename(predc1=pred)
  #predict using data in memory
  
rmse_cpub1<-modelr::rmse(cpub1,cwur);rmse_cpub1
  #shows the root mean squared average for the CWUR publication dataset prediction
```

##Shanghai Dataset Prediction


```{r Shanghai Publications Prediction}
spub1<-lm(shanghai$Publications~shanghai$`Highly Cited Researchers`,data=shanghai) 
  #publications ~(as a function of) HICI, data=dataset(shanghai)

#outcome (publications) on left, predictor (HICI/Citations) on right 
summary(spub1)#shows the results of the regression
```

RESULTS:
If the Highly Cited Researchers/Citations for the Shanghai Dataset is 0 then the intercept(publications) are predicted to be 28.33 (Est. Std).
As the Highly Cited researchers/citations increases within the Shanhai Dataset (every 1 unit change) publications are predicted to increase by .61 (Est. Std).

Fail to Reject the Null Hypothesis.

Residual Standard Error (RMSE): 9.641 on df(4893)

```{r Point Plot of Shanghai Research/Publications by HICI/Citations}
#The graph/plot below is pulling from the main dataset - shanghai in order to present the best point plot

gs2 <- ggplot(shanghai,aes(x=shanghai$`Highly Cited Researchers`,y=shanghai$Publications))+ 
      #x is the hici/citations and y is the publications
  geom_point(shape=3, alpha=.75, size=.25)+#specifies the points
  geom_smooth(method = "lm")+
  geom_smooth(method = "loess",color="darkgreen")+
  geom_smooth(color="grey")
gs2 <- gs2+ylab("Research/Publications")+xlab("HICI/Citations")#labels for x & y axis
gs2 <- gs2+ggtitle("Regression Lines: Research/Publications by HICI/Citations in the Shanghai Dataset")
gs2

##lm puts a straight line through the publication data points and is similiar to a best line fit.
##LOESS fits a curve through the publication data points and is similar to modeling with calculus as it is the weighted sum of squared erros and may accurately account for the range within the dataset.

```


Based on the data below the Root Mean Squared Error for the Shanghai prediction data for publications is 9.63, which means on average we are off by 10 points.

```{r RMSE for Shanghai publications}
#The code below runs the root mean squared error number from a validation of the model data above

shanghai<- shanghai%>%add_predictions(spub1)%>%rename(preds1=pred)
  #predict using data in memory
  
rmse_spub1<-modelr::rmse(spub1,shanghai);rmse_spub1
  #on average we are off by 9.63 points
```

##Times Dataset Prediction
```{r Times Publications Prediction}

tpub1<-lm(times$Research~times$Citations,data=times) 
  #research/pubs ~(as a function of) citations, data=dataset(times)

#outcome (research/pubs) on left, predictor (citations) on right 
summary(tpub1)#shows the results of the regression
```

RESULTS:
If the citations for the Times Dataset is 0 then the intercept(publications) are predicted to be 6.43 (Est. Std).
As citations increases within the Times Dataset (every 1 unit change) research/publications are predicted to increase by .48 points (Est. Std)

Fail to Reject the Null Hypothesis.

Residual Standard Error (RMSE): 18.09 on df(2601)

```{r Point Plot of Times Publications by Citations}
#The graph/plot below is pulling from the main dataset - shanghaiData in order to present the best point plot

gt2 <- ggplot(times,aes(x=times$Citations,y=times$Research))+ 
      #x is the world rank and y is the citations
  geom_point(shape=3, alpha=.75, size=.5)+#specifies the points
  geom_smooth(method = "lm")+
  geom_smooth(method = "loess",color="red")+
  geom_smooth(color="grey")
gt2 <- gt2+ylab("Research/Publications")+xlab("Citations")#labels for x & y axis
gt2 <- gt2+ggtitle("Regression Lines: Research/Publications by Citations in the Times Dataset")
gt2

##lm puts a straight line through the publication data points and is similiar to a best line fit.
##LOESS fits a curve through the publication data points and is similar to modeling with calculus as it is the weighted sum of squared erros and may accurately account for the range within the dataset.

```


Based on the data below the Root Mean Squared Error for the Times prediction data for citations is 18.08, which means on average we are off by 18 points.

```{r RMSE for Times publications}
#The code below runs the root mean squared error number from a validation of the model data above

times <- times%>%add_predictions(tpub1)%>%rename(predt1=pred)
  #predict using data in memory
  
rmse_tpub1 <- modelr::rmse(tpub1,times);rmse_tpub1
  #on average we are off by 12 points
```


```{r Prediction of the Publications under the 3 World Ranking Datasets}
#Prediction from the Publications data under the 3 world ranking datasets
rmse_cpub1
rmse_spub1
rmse_tpub1
```

When comparing the root mean squared error for each of the three global ranking datsets the margin of error appears to be greater with the CWUR dataset at 169 errors versus the Shanghai dataset at 10 errors and the Times dataset at 18 errors.  This tells us that there is a higher possibility of errors that one may receive within the CWUR dataset when comparing Publications versus Citations.





## **References**
Grolemund, G., & Wickham, H. (2017). R for Data Science (1st ed.). Sebastopol, CA: O’Rielly Media Inc. Retrieved from https://r4ds.had.co.nz/

Methodology | CWUR | Center for World University Rankings. (2012). Retrieved May 20, 2019, from https://cwur.org/methodology/world-university-rankings.php

Ranking Methodology of Academic Ranking of World Universities. (2015). Retrieved May 20, 2019, from http://www.shanghairanking.com/ARWU-Methodology-2015.html

World University Rankings 2015-2016 methodology. (2015). Retrieved May 20, 2019, from https://www.timeshighereducation.com/news/ranking-methodology-2016

World University Rankings DataSet. (2016). Retrieved May 20, 2019, from https://www.kaggle.com/mylesoneill/world-university-rankings

