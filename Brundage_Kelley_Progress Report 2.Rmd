---
title: "Progress Report 2"
Date: 12/June/2019
GitHub: https://github.com/klbrundage/LLO8200-Assignments
output:
  pdf_document: default
  html_document: default
  word_document: default
fontsize: 12pt
geometry: margin=.5
autosize: yes
Author: Kelley Brundage
---

```{r setup, include=FALSE}
##This code allows the Knit function to still work even with errors 
knitr::opts_chunk$set(echo=TRUE,error=TRUE)
options(tinytex.verbose=T)
```

```{r global_options, include = FALSE}
##This code does not show in the final document but will assist with definining the margin cutoff point and wraps the text to the next line.
knitr::opts_chunk$set(message=FALSE, 
tidy.opts=list(width.cutoff=60)) 

my_pdf = function(file,width,height)
{pdf(file, width=width, height=height,pointsize=12)}
```

```{r eval=F, tidy=T}
##The `eval=FALSE` option just displays the R code (and does not run it), `tidy=TRUE` wraps long code so it does not run off the page.

##Below are the standard set of setup commands and common libraries that may be needed when running data analysis as well as supporting table design and setup.

##The suppress messages option will stop the install.packages and library launches from showing in the console and instead will just load each library as called.
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library(dplyr))
suppressMessages(library(evaluate))
suppressMessages(library(forcats))
suppressMessages(library(formatR))
suppressMessages(library(ggplot2))
suppressMessages(library(haven))
suppressMessages(library(knitr))
suppressMessages(library(ModelMetrics))
suppressMessages(library(modelr))
suppressMessages(library(pander))
suppressMessages(library(readxl))
suppressMessages(library(RColorBrewer))
suppressMessages(library(stargazer))
suppressMessages(library(tables))
suppressMessages(library(tibble))
suppressMessages(library(tidyverse))
suppressMessages(library(xtable))
```

```{r Import World Ranking Datasets}
##The code below will upload the World University Ranking Datasets obtained from Kaggle.com

##The next set of files are the three World Ranking system files
library(readxl)
cwur <- read_excel("~/School/EdD program - Vanderbilt/01 - Classes/LLO 8200 - Intro to Data Science/Final Project/datafiles/cwurData.xlsx")

save(cwur, file = "cwurData.xlsx") #save as excel file name cwurData.xlsx

library(readxl)
shanghai <- read_csv("~/School/EdD program - Vanderbilt/01 - Classes/LLO 8200 - Intro to Data Science/Final Project/datafiles/shanghaiData.csv")

save(shanghai, file = "shanghaiData.csv") #save as csv file name shanghaiData.csv

library(readxl)
times <- read_excel("~/School/EdD program - Vanderbilt/01 - Classes/LLO 8200 - Intro to Data Science/Final Project/datafiles/timesData.xlsx")

save(times, file = "timesData.xlsx") #save as excel file name timesData.xlsx

##The three datasets below are the additional datasets provided that connect to the World University Ranking datasets
library(readxl)
education_expenditure_supplementary_data <- read_excel("~/School/EdD program - Vanderbilt/01 - Classes/LLO 8200 - Intro to Data Science/Final Project/datafiles/education_expenditure_supplementary_data.xlsx")

library(readxl)
educational_attainment_supplementary_data <- read_excel("~/School/EdD program - Vanderbilt/01 - Classes/LLO 8200 - Intro to Data Science/Final Project/datafiles/educational_attainment_supplementary_data.xlsx")

##The file below lists the name of the institution and the Country the institution is located in
library(readxl)
school_and_country_table <- read_excel("~/School/EdD program - Vanderbilt/01 - Classes/LLO 8200 - Intro to Data Science/Final Project/datafiles/school_and_country_table.xlsx")
```

# **Progress Report 2:  World University Rankings**

## *Synopsis of Problem & Approach from Progress Report 1*
Compare the three global ranking systems to the amount of faculty, publications/research at each institution per ranking system by approaching each dataset with an analytical and statistical viewpoint. 

Analyze the dataset specific to the area of research/academic and if common challenges that exist with all ranking systems exist.  Problems such as making the correction for institutional size, differences between average and extreme, defining the institutions, measurement of time frame, credit allocation, excellency factors as well as adjustment for scientific fields or types of research.

## *Columns in each dataset that will be used to compare and analysze the ranking system*

CWUR Dataset Fields:
world_rank; institution; country; national_rank; publications; citations

    Publications (measured by # of papers in top-tier journals - 15%)
    Citations

Shanghai Dataset Fields:
     word_rank; university_name; national_rank; pub; hici
     
     Pub (Publications)
     HICI (Highly Cited Researchers)
     
     Merged School and Country data into this file by adding a new column after University Name that indicates the Country.

Times Dataset Fields:
     world_rank; university; country; research; citations
     
     Research (volume, income, and reputation)
     Citations (research influence)

# **Extensive Investigation of Dataset**

Investigate data: distribution of data, correlations, associations, and predictive potential to solve your proposed problem:

Continued review and analysis of the datasets has led me to identify a series of common fields within the three primary ranking system datasets: CWUR, Shanghai and Times.  All three hold common columns such as the World Rank, Institution/University Name, Publications/Research and Citations.  My analysis will expand to look at both the research and citations between the three ranking systems as well as to compare the countries that appear in each dataset depending on the references within the datasets.

## Setting up the data frame
### *Data Clean-up*

```{r Tidy Principles}
#The code below will apply some baseline Tidy-Data principles which will allow future use of the clean data.

#1. Each variable forms a column
#2. Each observation forms a row
#3. Each type of observational units forms a table

is.data.frame(cwur)
is.tibble(cwur)
is_tibble(cwur)
typeof(cwur)

is.data.frame(shanghai)
is.tibble(shanghai)
is_tibble(shanghai)
typeof(shanghai)

is.data.frame(times)
is.tibble(times)
is_tibble(times)
typeof(times)
```

```{r CWUR Data Column labels}

## This code will clean up the column labels to align directly with the variable definitions for the columns as well as rename the columns into a readable format that describes the column data.

names(cwur)<-c("World Rank",
                "Instiution Name",
                "Country",
                "National Rank",
                "Quality of Education",
                "Alumni Employment",
                "Quality of Faculty",
                "Publications",
                "Influence",
                "Citations",
                "Broad Impact",
                "Patents",
                "Score",
                "Year")

head(cwur)
```

```{r Shanghai Data Column labels}

## This code will clean up the column labels to align directly with the variable definitions for the columns as well as rename the columns into a readable format that describes the column data.

names(shanghai)<-c("World Rank",
                "University Name",
                "Country",
                "National Rank",
                "Total Score",
                "Alumni",
                "Award",
                "Highly Cited Researchers",
                "Nature & Science Pubs",
                "Publications",
                "Per Capita Performance",
                "Year")

head(shanghai)
```

```{r Times Data Column labels}

## This code will clean up the column labels to align directly with the variable definitions for the columns as well as rename the columns into a readable format that describes the column data.

names(times)<-c("World Rank",
                "University Name",
                "Country",
                "Teaching",
                "International",
                "Research",
                "Citations",
                "Income",
                "Total Score",
                "Number of Students",
                "Student/Staff Ratio",
                "International Students",
                "Female/Male Ratio",
                "Year")

head(times)
```

##*Data Analysis*
Below you will find the mean of overall publications/citations for the three global ranking systems.  You will find that for all three datasets; CWUR, Shanghai and Times that on average 50 publications are produced.  The same holds true for cititations in that on average 50 citations are produced per institution.

### Regression Data for CWUR - Publications
```{r Regression for CWUR Data - Publications}
cwurp_mean <- cwur%>%summarise(mean(cwur$Publications,na.rm = T))
cwurp_mean
  #na.rm=T specifies what to do with any missing data in the dataset

cwur <- cwur%>%mutate(publications_p=percent_rank(cwur$Publications)*100)
  #default would give 0-1 so *by 100 and it will make it easier to understand the publication number in the CWUR dataset

cwurpp_mean <- cwur%>%summarise(mean(publications_p,na.rm = T))
cwurpp_mean
  #gives the mean of the publications in the CWUR dataset

#The code below shows Publications by Country in the CWUR dataset showing the mean, standard deviation, max and min.
desc_cwur <- cwur%>%
  group_by(Country)%>%
  summarise(mean_pub = mean(publications_p),
            sd_pub = sd(publications_p),
            max_pub= max(publications_p),
            min_pub= min(publications_p))
desc_cwur

#The code below shows which country has the maximum and minimum Publications inthe CWUR dataset
desc_cwur%>%
  filter(max(mean_pub) == mean_pub)

desc_cwur%>%
  filter(min(mean_pub) == mean_pub)
```

Results for CWUR Publications:

Average Publications in CWUR dataset: 460
Average Publications by percent rank in CWUR dataset: 50
Publications by Country in the CWUR dataset showing the mean, standard deviation, max and min.
Filter showing the Country with the Maximum average publications: Puerto Rico
Filter showing the Country with the Minimum average publications: Singapore

### Regression Data for CWUR - Citations
```{r Regression for CWUR Data - Citations}

cwurc_mean <- cwur%>%summarise(mean(cwur$Citations,na.rm = T))
cwurc_mean
  #na.rm=T specifies what to do with any missing data in the dataset

cwur <- cwur%>%mutate(citations_p=percent_rank(cwur$Citations)*100)
  #default would give 0-1 so *by 100 and it will make it easier to understand the publication number in the CWUR dataset

cwurcp_mean <- cwur%>%summarise(mean(citations_p,na.rm = T))
cwurcp_mean
  #gives the mean of the publications in the CWUR dataset

#The code below shows Citations by Country in the CWUR dataset showing the mean, standard deviation, max and min.
desc_cwur <- cwur%>%
  group_by(Country)%>%
  summarise(mean_cit = mean(Citations),
            sd_cit = sd(Citations),
            max_cit= max(Citations),
            min_cit= min(Citations))
desc_cwur

#The code below shows which country has the maximum and minimum Citations inthe CWUR dataset
desc_cwur%>%
  filter(max(mean_cit) == mean_cit)

desc_cwur%>%
  filter(min(mean_cit) == mean_cit)
```

Results for CWUR Citations:

Average Citations in CWUR dataset: 413
Average Citations by percent rank in CWUR dataset:48
Citations by Country in the CWUR dataset showing the mean, standard deviation, max and min.
Filter showing the Country with the Maximum average citations: Lebanon, United Arab Emirates, Uruguay
Filter showing the Country with the Minimum average citations: Singapore

### Regression Data for Shanghai - Publications
```{r Regression for Shanghai Data - Publications}

spub_mean <- shanghai%>%summarise(mean(shanghai$Publications,na.rm = T))
spub_mean
  #na.rm=T specifies what to do with any missing data in the dataset

shanghai <- shanghai%>%mutate(publications_p=percent_rank(shanghai$Publications)*100)
  #default would give 0-1 so *by 100 and it will make it easier to understand the publication number in the Shanghai dataset

spubp_mean <- shanghai%>%summarise(mean(publications_p,na.rm = T))
spubp_mean
  #gives the mean of the publications in the Shanghai dataset

#The code below shows Citations by National Rank in the Shanghai dataset showing the mean, standard deviation, max and min.
desc_shanghai <- shanghai%>%
  group_by(shanghai$Country)%>%
  summarise(mean_spub = mean(shanghai$Publications),
            sd_spub = sd(shanghai$Publications),
            max_spub = max(shanghai$Publications),
            min_spub = min(shanghai$Publications))
desc_shanghai

#The code below shows which country has the maximum and minimum Citations in the Shanghai dataset
desc_shanghai%>%
  filter(max(mean_spub) == mean_spub)

desc_shanghai%>%
  filter(min(mean_spub) == mean_spub)
```

Results for Shanghai Publications:

Average Publications in Shanghai dataset: 38.25
Average Publications by percent rank in Shanghai dataset:50
Publications by Country in the Shanghai dataset showing the mean, standard deviation, max and min.
Filter showing the Country with the Maximum average publications: 
Filter showing the Country with the Minimum average publications: 

### Regression Data for Shanghai - HICI/Citations
```{r Regression for Shanghai Data - Highly Cited Researchers}

shici_mean <- shanghai%>%summarise(mean(shanghai$`Highly Cited Researchers`,na.rm = T))
shici_mean
  #na.rm=T specifies what to do with any missing data in the dataset

shanghai <- shanghai%>%mutate(hici_p=percent_rank(shanghai$`Highly Cited Researchers`)*100)
  #default would give 0-1 so *by 100 and it will make it easier to understand the publication number in the Shanghai dataset

shicip_mean <- shanghai%>%summarise(mean(hici_p,na.rm = T))
shicip_mean
  #gives the mean of the publications in the Shanghai dataset

#The code below shows Citations by National Rank in the Shanghai dataset showing the mean, standard deviation, max and min.
desc_shanghai <- shanghai%>%
  group_by(shanghai$Country)%>%
  summarise(mean_shici = mean(shanghai$`Highly Cited Researchers`),
            sd_shici = sd(shanghai$`Highly Cited Researchers`),
            max_shici = max(shanghai$`Highly Cited Researchers`),
            min_shici = min(shanghai$`Highly Cited Researchers`))
desc_shanghai

#The code below shows which country has the maximum and minimum Citations in the Shanghai dataset
desc_shanghai%>%
  filter(max(mean_shici) == mean_shici)

desc_shanghai%>%
  filter(min(mean_shici) == mean_shici)
```

Results for Shangha HICI/Citations:

Average HICI/Citations in Shanghai dataset: 16.22
Average HICI/Citations by percent rank in Shanghai dataset:48
HICI/Citations by Country in the Shanghai dataset showing the mean, standard deviation, max and min.
Filter showing the Country with the Maximum average citations: 
Filter showing the Country with the Minimum average citations: 

### Regression Data for Times - Research/Publications
```{r Regression for Times Data - Publications/Research}

timesr_mean <- times%>%summarise(mean(times$Research,na.rm = T))
timesr_mean
  #na.rm=T specifies what to do with any missing data in the dataset

times <- times%>%mutate(research_p=percent_rank(times$Research)*100)
  #default would give 0-1 so *by 100 and it will make it easier to understand the publication number in the Times dataset

timesrp_mean <- times%>%summarise(mean(research_p,na.rm = T))
timesrp_mean
  #gives the mean of the publications in the Times dataset

#The code below shows Publications/Research by Country in the Times dataset showing the mean, standard deviation, max and min.
desc_times <- times%>%
  group_by(Country)%>%
  summarise(mean_tpub = mean(times$Research),
            sd_tpub = sd(times$Research),
            max_tpub= max(times$Research),
            min_tpub= min(times$Research))
desc_times

#The code below shows which country has the maximum and minimum Publications/Research in the Times dataset
desc_times%>%
  filter(max(mean_tpub) == mean_tpub)

desc_times%>%
  filter(min(mean_tpub) == mean_tpub)
```

Results for Times Publications:

Average Publications in Times dataset: 35.91
Average Publicatinos by percent rank in Times dataset:50
Publications by Country in the Times dataset showing the mean, standard deviation, max and min.
Filter showing the Country with the Maximum average publications:
Filter showing the Country with the Minimum average publications: 

### Regression Data for Times - Citations
```{r Regression for Times Data - Citations}

timesc_mean <- times%>%summarise(mean(times$Citations,na.rm = T))
timesc_mean
  #na.rm=T specifies what to do with any missing data in the dataset

times <- times%>%mutate(citations_p=percent_rank(times$Citations)*100)
  #default would give 0-1 so *by 100 and it will make it easier to understand the publication number in the Times dataset

timescp_mean <- times%>%summarise(mean(citations_p,na.rm = T))
timescp_mean
  #gives the mean of the publications in the Times dataset

#The code below shows Publications/Research by Country in the Times dataset showing the mean, standard deviation, max and min.
desc_times <- times%>%
  group_by(Country)%>%
  summarise(mean_tcit = mean(times$Citations),
            sd_tcit = sd(times$Citations),
            max_tcit= max(times$Citations),
            min_tcit= min(times$Citations))
desc_times

#The code below shows which country has the maximum and minimum Publications/Research in the Times dataset
desc_times%>%
  filter(max(mean_tcit) == mean_tcit)

desc_times%>%
  filter(min(mean_tcit) == mean_tcit)
```

Results for Times Publications:

Average Publications in Times dataset: 35.91
Average Publicatinos by percent rank in Times dataset:50
Publications by Country in the Times dataset showing the mean, standard deviation, max and min.
Filter showing the Country with the Maximum average publications:
Filter showing the Country with the Minimum average publications:

##Summary of Regression Data Above

```{r Summary of three ranking systems regression data}

#The code below provides a table view for each of the regression data calculations dones above for the three world ranking datasets: CWUR, Shanghai and Times

cwurp_mean
cwurpp_mean
cwurc_mean
cwurcp_mean
desc_cwur

spub_mean
spubp_mean
shici_mean
shicip_mean
desc_shanghai

timesr_mean
timesrp_mean
timesc_mean
timescp_mean
desc_times
```

## *Visuals: Density Plots & Scatterplots*

Below are a baseline set of Density plots that show the regression data for overall publications and citations within the three world ranking datasets.

```{r Density Plot of Citations in CWUR Dataset}

gc1 <- ggplot(cwur,aes(x=cwur$Citations))
gc1 <- gc1+geom_density(binwidth = 1,fill="lightblue") #Density is the shape
gc1 <- gc1+ylab("Count")+xlab("Citations") ## x & y axis labels
gc1 <- gc1+theme(axis.text.x = element_text(angle = 60, hjust = 1))
gc1 <- gc1+ggtitle("Citations in CWUR Dataset") ## Chart Title
gc1
```

Density Plot #1: Citations in CWUR Daset
The plot above shows the level of citations referenced within the CWUR dataset.  This chart reflects that the mid-way of 400 citations appears to be the average with two outlier areas of a low point between 0-100 citations and a high point between 700-800 citations.

```{r Density Plot of Citations in Shanghai Dataset}
gs1 <- ggplot(shanghai,aes(x=shanghai$`Highly Cited Researchers`))
gs1 <- gs1+geom_density(binwidth = 1,fill="lightgreen") #Density is the shape
gs1 <- gs1+ylab("Count")+xlab("Citations") ## x & y axis labels
gs1 <- gs1+theme(axis.text.x = element_text(angle = 60, hjust = 1))
gs1 <- gs1+ggtitle("Citations in Shanghai Dataset") ## Chart Title
gs1
```

Density Plot #2: Citations in Shanghai Daset
The plot above shows the level of citations referenced within the Shanghai dataset.  You will notice that the peak of citations within this dataset is on average 10-12 or a little under the mid-way mark between 0-25 citations.

```{r Density Plot of Citations in Times Dataset}

gt1 <- ggplot(times,aes(x=times$Citations))
gt1 <- gt1+geom_density(binwidth = 1,fill="lightyellow") #Density is the shape
gt1 <- gt1+ylab("Count")+xlab("Citations") ## x & y axis labels
gt1 <- gt1+theme(axis.text.x = element_text(angle = 60, hjust = 1))
gt1 <- gt1+ggtitle("Citations in Times Dataset") ## Chart Title
gt1
```

Density Plot #3: Citations in Times Daset
The plot above shows the level of citations referenced within the Times dataset.  You will n otice that the average or peak citations is at the middle mark between 50-75 citations.

##*World Rankings Dataset Predictions*

The data and charts below show the pulication/citation predictions based on the three world datasets.

## *CWUR Dataset Prediction*

### Publications
```{r CWUR Publications Prediction}

cpub1<-lm(cwur$Publications~cwur$Citations,data=cwur) 
  #publications ~(as a function of) citations, data=dataset(cwur)

#outcome (publications) on left, predictor (citations) on right 
summary(cpub1)#shows the results of the regression
```

RESULTS:
F-test = 4864       dfn=1  dfd=2196       fcv=3.84
2196>3.84 so we reject the Ho
p-value = 2.2e-16 > .5 so we fail to reject the Ho
R-squared: .69

A simple linear regression analysis shows that the activity for publicatinos can be significantly predicted based on the citations produced, F(1,2196)=3.84, p>.05, R2 = .69

If the Citations for the CWUR Dataset is 0 then the intercept(publications) are predicted to be 65.68 (Est. Std). As citations increases within the CWUR Dataset (every 1 unit change) publications are predicted to increase by .95 points (Est. Std)

Residual Standard Error (RMSE): 169.5 on df(2198)

### Citations
```{r Point Plot of CWUR Publications by Citations}
#The graph/plot below is pulling from the main dataset - CWUR in order to present the best point plot

gc2 <- ggplot(cwur,aes(x=cwur$Citations, y=cwur$Publications))+
  geom_point(shape=2, alpha=.5, size=.5)+
 geom_smooth(method = "lm")+
  geom_smooth(method = "loess",color="darkblue")+
  geom_smooth(color="grey")
gc2 <- gc2+ylab("Publications")+xlab("Citations")
gc2 <- gc2+ggtitle("Regression Lines: Number of Publications by Citations made in CWUR Dataset")
gc2

##lm puts a straight line through the publication data points and is similiar to a best line fit.
##LOESS fits a curve through the publication data points and is similar to modeling with calculus as it is the weighted sum of squared erros and may accurately account for the range within the dataset.

```

## RMSE for CWUR Publications
Based on the data below the Root Mean Squared Error for the CWUR prediction data for publications is 169.43, which means on average we are off by 169.

```{r RMSE for CWUR publications}
#The code below runs the root mean squared error number from a validation of the model data above

cwur<- cwur%>%add_predictions(cpub1)%>%rename(predc1=pred)
  #predict using data in memory
  
rmse_cpub1<-modelr::rmse(cpub1,cwur);rmse_cpub1
  #shows the root mean squared average for the CWUR publication dataset prediction
```

### Coefficient Data for CWUR Publications
```{r Summary Data for CWUR Publications}
confint.lm(cpub1)
#This code only shows the coefficient Data
```

At 169 Citations on average the predicted amount of publication activity would be:
Y = a+bX
Y = 52.5259090 + .9267652(169)
Y = 52.5259090 + 156.6233
Y = 209.1492

The 95% CI for the slope [.93,.98] does not contain 0 so we reject the null hypothesis for the slope.

##Shanghai Dataset Prediction

### Publications
```{r Shanghai Publications Prediction}
spub1<-lm(shanghai$Publications~shanghai$`Highly Cited Researchers`,data=shanghai) 
  #publications ~(as a function of) HICI, data=dataset(shanghai)

#outcome (publications) on left, predictor (HICI/Citations) on right 
summary(spub1)#shows the results of the regression
```

RESULTS:
F-test = 4075       dfn=1  dfd=4891       fcv=3.84
4075>3.84 so we reject the Ho
p-value = 2.2e-16 > .5 so we fail to reject the Ho
R-squared: .45

A simple linear regression analysis shows that the activity for publicatinos can be significantly predicted based on the citations produced, F(1,4891)=3.84, p>.05, R2 = .45

If the Highly Cited Researchers/Citations for the Shanghai Dataset is 0 then the intercept(publications) are predicted to be 28.33 (Est. Std). As the Highly Cited researchers/citations increases within the Shanhai Dataset (every 1 unit change) publications are predicted to increase by .61 (Est. Std).

Residual Standard Error (RMSE): 9.641 on df(4893)

### HICI/Citations
```{r Point Plot of Shanghai Research/Publications by HICI/Citations}
#The graph/plot below is pulling from the main dataset - shanghai in order to present the best point plot

gs2 <- ggplot(shanghai,aes(x=shanghai$`Highly Cited Researchers`,y=shanghai$Publications))+ 
      #x is the hici/citations and y is the publications
  geom_point(shape=3, alpha=.75, size=.25)+#specifies the points
  geom_smooth(method = "lm")+
  geom_smooth(method = "loess",color="darkgreen")+
  geom_smooth(color="grey")
gs2 <- gs2+ylab("Research/Publications")+xlab("HICI/Citations")#labels for x & y axis
gs2 <- gs2+ggtitle("Regression Lines: Research/Publications by HICI/Citations in the Shanghai Dataset")
gs2

##lm puts a straight line through the publication data points and is similiar to a best line fit.
##LOESS fits a curve through the publication data points and is similar to modeling with calculus as it is the weighted sum of squared erros and may accurately account for the range within the dataset.

```

The scatterplot above with the Linear and LOESS regression lines is showing a positive relationship 

### RMSE for Shanghai Publications
Based on the data below the Root Mean Squared Error for the Shanghai prediction data for publications is 9.63, which means on average we are off by 10 points.

```{r RMSE for Shanghai publications}
#The code below runs the root mean squared error number from a validation of the model data above

shanghai<- shanghai%>%add_predictions(spub1)%>%rename(preds1=pred)
  #predict using data in memory
  
rmse_spub1<-modelr::rmse(spub1,shanghai);rmse_spub1
  #on average we are off by 9.63 points
```

### Coefficient Data for Shanghai Publications
```{r Summary Data for Shanghai Publications}
confint.lm(spub1)
#This code only shows the coefficient Data
```

At 10 Citations on average the predicted amount of publication activity would be:
Y = a+bX
Y = 25.9254871 + .5928685(10)
Y = 25.9254871 + 5.928685
Y = 31.85417

The 95% CI for the slope [.60,.63] does not contain 0 so we reject the null hypothesis for the slope.

##Times Dataset Prediction

### Publications
```{r Times Publications Prediction}

tpub1<-lm(times$Research~times$Citations,data=times) 
  #research/pubs ~(as a function of) citations, data=dataset(times)

#outcome (research/pubs) on left, predictor (citations) on right 
summary(tpub1)#shows the results of the regression
```

RESULTS:
F-test = 991.1       dfn=1  dfd=2598       fcv=3.84
991.1>3.84 so we reject the Ho
p-value = 2.2e-16 > .5 so we fail to reject the Ho
R-squared: .2759

A simple linear regression analysis shows that the activity for publicatinos can be significantly predicted based on the citations produced, F(1,2599)=3.84, p>.05, R2 = .28

If the citations for the Times Dataset is 0 then the intercept(publications) are predicted to be 6.43 (Est. Std). As citations increases within the Times Dataset (every 1 unit change) research/publications are predicted to increase by .48 points (Est. Std)

Residual Standard Error (RMSE): 18.09 on df(2601)

### Citations
```{r Point Plot of Times Publications by Citations}
#The graph/plot below is pulling from the main dataset - shanghaiData in order to present the best point plot

gt2 <- ggplot(times,aes(x=times$Citations,y=times$Research))+ 
      #x is the world rank and y is the citations
  geom_point(shape=3, alpha=.75, size=.5)+#specifies the points
  geom_smooth(method = "lm")+
  geom_smooth(method = "loess",color="red")+
  geom_smooth(color="grey")
gt2 <- gt2+ylab("Research/Publications")+xlab("Citations")#labels for x & y axis
gt2 <- gt2+ggtitle("Regression Lines: Research/Publications by Citations in the Times Dataset")
gt2

##lm puts a straight line through the publication data points and is similiar to a best line fit.
##LOESS fits a curve through the publication data points and is similar to modeling with calculus as it is the weighted sum of squared erros and may accurately account for the range within the dataset.

```

### RMSE for Times Publications
Based on the data below the Root Mean Squared Error for the Times prediction data for citations is 18.08, which means on average we are off by 18 points.

```{r RMSE for Times publications}
#The code below runs the root mean squared error number from a validation of the model data above

times <- times%>%add_predictions(tpub1)%>%rename(predt1=pred)
  #predict using data in memory
  
rmse_tpub1 <- modelr::rmse(tpub1,times);rmse_tpub1
  #on average we are off by 12 points
```

### Coefficient Data for Times Publications
```{r Summary Data for Times Publications}
confint.lm(tpub1)
#This code only shows the coefficient Data
```

At 18 Citations on average the predicted amount of publication activity would be:
Y = a+bX
Y = 4.4689367 + .4537283(18)
Y = 4.4689367 + 8.167109
Y = 12.63605

The 95% CI for the slope [.45,.51] does not contain 0 so we reject the null hypothesis for the slope.

## Summary RMSE data for World Ranking Dataset Publications
```{r Prediction of the Publications under the 3 World Ranking Datasets}
#Prediction from the Publications data under the 3 world ranking datasets
rmse_cpub1
rmse_spub1
rmse_tpub1
```

## *Summary*
When comparing the root mean squared error for each of the three global ranking datsets the margin of error appears to be greater with the CWUR dataset at 169 errors versus the Shanghai dataset at 10 errors and the Times dataset at 18 errors.  This tells us that there is a higher possibility of errors that one may receive within the CWUR dataset when comparing Publications versus Citations.


## **References**
Grolemund, G., & Wickham, H. (2017). R for Data Science (1st ed.). Sebastopol, CA: O'Rielly Media Inc. Retrieved from https://r4ds.had.co.nz/

Methodology | CWUR | Center for World University Rankings. (2012). Retrieved May 20, 2019, from https://cwur.org/methodology/world-university-rankings.php

Ranking Methodology of Academic Ranking of World Universities. (2015). Retrieved May 20, 2019, from http://www.shanghairanking.com/ARWU-Methodology-2015.html

World University Rankings 2015-2016 methodology. (2015). Retrieved May 20, 2019, from https://www.timeshighereducation.com/news/ranking-methodology-2016

World University Rankings DataSet. (2016). Retrieved May 20, 2019, from https://www.kaggle.com/mylesoneill/world-university-rankings

